{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier transformation\n",
    "\n",
    "This notebook is responsible for converting the eeg data stored in the database into images representing a second of data.\n",
    "\n",
    "The data is transormed using fourier transformation, and each timeframe represents a row in the final image. To accomodate for the complex numbers produces using fourier transformation, the real part is placed in the red channel of the image, and the imaginary part is placed in the green. The blue channel is unused and is filled with zero. \n",
    "\n",
    "To make a simple test on training with this data, the dataset is turned into a classification problem to see if the pasient is working with their left or right side, this is divided into 4 labels\n",
    "\n",
    "1. none - used for baseline recordings, where they are instructed to lie still\n",
    "2. left - they move their left hand\n",
    "3. right - they move their right hand\n",
    "4. both - they move both hands or feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sql_data_manager as sql\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = sql.SqlDataSet(\"no_baseline_model.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "REST_POSITION = 1\n",
    "\n",
    "OPEN_CLOSE_LEFT_FIST_ACTUAL = 2\n",
    "OPEN_CLOSE_LEFT_FIST_IMAGINED = 3\n",
    "OPEN_CLOSE_RIGHT_FIST_ACTUAL = 4\n",
    "OPEN_CLOSE_RIGHT_FIST_IMAGINED = 5\n",
    "OPEN_CLOSE_BOTH_FISTS_ACTUAL = 6\n",
    "OPEN_CLOSE_BOTH_FISTS_IMAGINED = 7\n",
    "OPEN_CLOSE_BOTH_FEET_ACTUAL = 8\n",
    "OPEN_CLOSE_BOTH_FEET_IMAGINED = 9\n",
    "\n",
    "def is_both(v: torch.Tensor) -> bool:\n",
    "    return OPEN_CLOSE_BOTH_FISTS_ACTUAL/9.0 in v or OPEN_CLOSE_BOTH_FEET_ACTUAL/9.0 in v \n",
    "\n",
    "def is_left(v: torch.Tensor) -> bool:\n",
    "    return OPEN_CLOSE_LEFT_FIST_ACTUAL/9.0 in v or is_both(v)\n",
    "\n",
    "def is_right(v: torch.Tensor) -> bool:\n",
    "    return OPEN_CLOSE_RIGHT_FIST_ACTUAL/9.0 in v or is_both(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [01:04<00:00, 20.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from os import makedirs\n",
    "\n",
    "\n",
    "data = []\n",
    "window_size = 160\n",
    "# good overlap\n",
    "step_size = window_size // 3\n",
    "\n",
    "made_dirs = set()\n",
    "for recording in tqdm(range(len(database))):\n",
    "    signals, labels = database[recording]\n",
    "\n",
    "    for chunk in range(0, signals.shape[0], step_size):\n",
    "        start = chunk*step_size\n",
    "        end = chunk*step_size + window_size\n",
    "        view = signals[start:end, :, :]\n",
    "        if view.shape[0] != 160:\n",
    "            continue\n",
    "        \n",
    "        label_view = labels[start:end, :]\n",
    "\n",
    "        transformed = torch.fft.fft(view.view(-1,64))\n",
    "        red = transformed.real.view(1, -1, 64,)\n",
    "        green = transformed.imag.view(1, -1, 64,)\n",
    "        blue = torch.zeros(1, transformed.shape[0], 64,)\n",
    "        \n",
    "        merged = (torch.cat((red, green, blue), 0)*255).byte()\n",
    "\n",
    "        if merged.shape[1] < 160:\n",
    "            print(merged.shape)\n",
    "        #merged = torch.nn.functional.pad(merged, ())\n",
    "\n",
    "        left = is_left(label_view)\n",
    "        right = is_right(label_view)\n",
    "\n",
    "        label = \"\"\n",
    "        match (left, right):\n",
    "            case (False, False):\n",
    "                label = \"none\"\n",
    "            case (True, False):\n",
    "                label = \"left\"\n",
    "            case (False, True):\n",
    "                label = \"right\"\n",
    "            case (True, True):\n",
    "                label = \"both\"\n",
    "\n",
    "        dir = f\"./data/generated/{label}\"\n",
    "        filename = f\"{dir}/r{recording}o{chunk}.png\"\n",
    "        if dir not in made_dirs:\n",
    "            made_dirs.add(dir)\n",
    "            makedirs(dir, exist_ok=True)\n",
    "\n",
    "        torchvision.io.write_png(merged, filename)\n",
    "\n",
    "        # plt.imshow(transformed[0,:].real)\n",
    "        # plt.figure()\n",
    "        # plt.imshow(transformed[0,:].imag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
